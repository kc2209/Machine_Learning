{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfH8P7+c8gDZN/AdWHyYS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kc2209/Machine_Learning/blob/main/Datapreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "Y5NYMmGvvwoH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xULR3-2yu1km"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing the dataset(First upload the file into google colab using files menu)"
      ],
      "metadata": {
        "id": "U5ZldAjUwHiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Data_preprocessing.csv\")"
      ],
      "metadata": {
        "id": "4W32JENMyH94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to create essential elements for machine learning models i.e., Feature matrix and Dependent variable vector.\n",
        "Feature matrix holds independent features or predictors. These are the characteristics you will use to build a model that predicts the dependent variable.\n",
        "The dependent variable vector stores the dependent variable also called target variable or response variable.\n",
        "x is feature matrix, y is dependent variable vector."
      ],
      "metadata": {
        "id": "9iaTnYxSym_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = df.iloc[:,:-1].values\n",
        "y = df.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "bDNlUOMu0vj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset has four columns, in which Country, age and salary are independent variables or features. Purchased is the dependent variable."
      ],
      "metadata": {
        "id": "fsjkLb6y1ezM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python's iloc() function is an important tool in Pandas for data manipulation. It allows the selection and retrieval of specific rows and columns in DataFrames or Series using integer-based indexing. iloc() allows to identification data by specifying row and column indices numerically.\n",
        "For reference see: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html"
      ],
      "metadata": {
        "id": "lijQhTdi4Ni8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ".values converts the selected subset array into numpy array.\n",
        "Here -1 is because that the dependent variable 'Purchase' is last column."
      ],
      "metadata": {
        "id": "MolQ6o2C4kSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clhr5Qcq5IDK",
        "outputId": "fdbd4317-66b8-48e4-d01b-6d3b23e99c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 nan]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' nan 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n",
            "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Addressing Missing Values:"
      ],
      "metadata": {
        "id": "hOzkPH8t5-yN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputation is one of the common techniques used to deal with missing values, which replaces the missing value with estimates i.e., mean or median or mode."
      ],
      "metadata": {
        "id": "UY3k13eW6PV4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do this imputation using **Scikit-learn** library which provides the SimpleImputer class for various imputation strategies."
      ],
      "metadata": {
        "id": "g7xA9Yya7Oxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(missing_values=np.nan,strategy='mean')\n",
        "imputer.fit(x[:,1:3])\n",
        "x[:,1:3]= imputer.transform(x[:,1:3])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlUgA43G77oE",
        "outputId": "1c046d35-5513-42fd-92c2-ab9b928f5bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['France' 44.0 72000.0]\n",
            " ['Spain' 27.0 48000.0]\n",
            " ['Germany' 30.0 54000.0]\n",
            " ['Spain' 38.0 61000.0]\n",
            " ['Germany' 40.0 63777.77777777778]\n",
            " ['France' 35.0 58000.0]\n",
            " ['Spain' 38.77777777777778 52000.0]\n",
            " ['France' 48.0 79000.0]\n",
            " ['Germany' 50.0 83000.0]\n",
            " ['France' 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing_values argument tells the SimpleImputer to identify missing values. Here np.nan which means not a number is used as an indicator for missing values, which is a common representation of missing values in numpy arrays which are often used in machine learning."
      ],
      "metadata": {
        "id": "kvfAuQrp9iHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Categorical Data"
      ],
      "metadata": {
        "id": "XKGOYIsoNCD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step is to Encode categorical data. Since many machine learning algorithms primarily work with numerical data. Encoding is transforming the categorical data to numerical data. We have a popular method called \"one-hot Encoding\".\n",
        "Lets see how we can apply this method to the \"country\" column using scikit-learn library."
      ],
      "metadata": {
        "id": "Ez8Bue-_-t1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])],remainder='passthrough')\n",
        "x = np.array(ct.fit_transform(x))\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3hbkEcwK9Fg",
        "outputId": "6a051686-5076-4fe5-c159-fc02ca2bd9a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [0.0 1.0 0.0 30.0 54000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 35.0 58000.0]\n",
            " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ColumnTransformer enable us to apply different transformations to specific columns of our data.\n",
        "\n",
        "*  In the transformer parameter, First we specifiy which type of transformation we want to apply. This value is arbitary. OneHotEncoder() specifies that its onehotencoder class we want to apply. And [0] indicates that onehotencoder will be applied only to the first column.   \n",
        "*   Second parameter, remainder = 'passthrough' tells the ColumnTransformer function to leave the columns which are not mentioned, unchanged.\n",
        "\n",
        "Now ct means that we want to apply onehotencoder tranformation only to the first column of our data and leave the other columns unchnaged.\n",
        "\n",
        "Now to apply this ct to our data 'x', use use fit_transform(x). Fit_transform belongs to ColumnTransfer class which performs two functionalities. Firstly,Fit function examines the 1st column of our data(since we specified only 1st column) and determine which unique values exist to perform onehotencoding.Next is tranform: Once equippped with the knowledge of the data, it applies the designated transformations to the specifies columns.\n",
        "\n",
        "Then np.array converts the resulted tranformed data to numpy array.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "16ncaZhCbm4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "y = LabelEncoder().fit_transform(y)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnenP1-5gCsp",
        "outputId": "0fb16aa8-6471-4e60-bb22-ee920aa079aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0 0 1 1 0 1 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPlitting the data:\n",
        "\n",
        "Next our step is to split the data into training set and testing set.\n",
        "\n",
        "\n",
        "This can be done using a function from scikit learn library called \"train_test_split\" within its model_selection module.\n",
        "\n",
        "It generates two pairs:\n",
        "\n",
        "*   A training set containing features(X_train) and target variables(y_train)\n",
        "*   A testing set containing features(X_test) and target variables(y_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "SKw2PZCeinOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# create four variables: x_train,x_test,y_train,y_test\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state=1)"
      ],
      "metadata": {
        "id": "TsFM3gnPj1Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "x,y represents our feature matrix and target variable. test_size = 0.2 indicates that 20% of the data will be set for testing.\n",
        "random_state parameter is an optional parameter which controls the randomness in shuffling the data before splitting. random_state = 1 ensures that everytime we run this code we get the same random split. The number is like an identification mark. This number is arbitary."
      ],
      "metadata": {
        "id": "kn98_nOApbSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train,'\\n',x_test,'\\n',y_train,'\\n',y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKP5-elDpnFr",
        "outputId": "4d3f44ac-5ca8-402f-e933-a25f43c03793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
            " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
            " [1.0 0.0 0.0 44.0 72000.0]\n",
            " [0.0 0.0 1.0 38.0 61000.0]\n",
            " [0.0 0.0 1.0 27.0 48000.0]\n",
            " [1.0 0.0 0.0 48.0 79000.0]\n",
            " [0.0 1.0 0.0 50.0 83000.0]\n",
            " [1.0 0.0 0.0 35.0 58000.0]] \n",
            " [[0.0 1.0 0.0 30.0 54000.0]\n",
            " [1.0 0.0 0.0 37.0 67000.0]] \n",
            " [0 1 0 0 1 1 0 1] \n",
            " [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling:\n",
        "\n",
        "we will use \"standardScaler\" class from preprocessing module, which facilitates standardization on feature matrices of both training set and test set."
      ],
      "metadata": {
        "id": "gGixpChQqDg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#Lets create an object or variable and call it sc to perform standardization\n",
        "sc = StandardScaler()\n",
        "# Note that we dont apply this feature scaling to dummy variables i.e., the features we got after encoding the country column which only has either 0 or 1. We dont do feature\n",
        "# scaling to them. Instead we do for other continuous variables such as age etc.\n",
        "x_train[:,3:] = sc.fit_transform(x_train[:,3:])\n",
        "# .fit evaluates the features and compute mean and standrad deviation of the features and transform performs the trnsformation of the these values using mean and SD\n",
        "x_test[:,3:] = sc.transform(x_test[:,3:])\n",
        "# here we are using only transform, instead of fit_transform. Here the transform function transforms the testing data using the same mean and SD  calculated by fit above.\n",
        "# this ensures the consistency in transformation and prevents data leakage."
      ],
      "metadata": {
        "id": "pRySEyIEriaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)\n",
        "print(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXm-ZO62vlNJ",
        "outputId": "5e9f4ac6-3c17-44b5-e46f-19dcab569180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
            " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
            " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
            " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
            " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
            " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
            " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
            " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
            "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
            " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is ready and now we are ready to build the machine learning algorithms."
      ],
      "metadata": {
        "id": "P8UBpocKvxpF"
      }
    }
  ]
}